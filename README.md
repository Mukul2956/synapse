# ORBIT â€” Intelligent Distribution & Scheduling Nexus

> Part of the **SYNAPSE** AI-Powered Content Intelligence Platform  
> Built for hackathon. ORBIT is the ðŸ”µ distribution pillar.

---

## What ORBIT does

| Feature | Description |
|---------|-------------|
| **Adaptive Timing Engine** | Uses Facebook Prophet + audience pattern data to predict the optimal posting slot per user Ã— platform |
| **Content Queue Intelligence** | Priority queue with relevance decay; time-sensitive content floats, evergreen sinks slowly |
| **Cross-Platform Orchestrator** | Coordinates sequential publishing to Twitter, Instagram, LinkedIn, Facebook, YouTube with strategic delays |
| **Algorithm Monitor** | Z-score + Welch's t-test anomaly detection; flags when a platform likely changed its algorithm |
| **Repurposing Engine** | Scores evergreen content and automatically re-queues it after a configurable interval |

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| API | FastAPI 0.104 + Uvicorn |
| Task Queue | Celery 5 + Redis |
| Scheduling | APScheduler / Celery Beat |
| Database | PostgreSQL 15 + TimescaleDB (time-series) |
| ML | Prophet (timing), LightGBM (priority), Scikit-learn (patterns) |
| Platform SDKs | Tweepy v4, Facebook Graph API, LinkedIn API v2, YouTube Data API v3 |
| Containerisation | Docker + Docker Compose |

---

## Project Structure

```
orbit/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                # FastAPI app
â”‚   â”œâ”€â”€ config.py              # Pydantic settings
â”‚   â”œâ”€â”€ database.py            # Async SQLAlchemy engine
â”‚   â”œâ”€â”€ models/                # SQLAlchemy ORM models
â”‚   â”œâ”€â”€ schemas/               # Pydantic request/response schemas
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ timing_engine.py   # Prophet-based optimal time predictor
â”‚   â”‚   â”œâ”€â”€ queue_manager.py   # Queue CRUD + decay
â”‚   â”‚   â”œâ”€â”€ orchestrator.py    # Cross-platform publish workflow
â”‚   â”‚   â”œâ”€â”€ algorithm_monitor.py # Platform change detector
â”‚   â”‚   â””â”€â”€ repurposing_engine.py # Evergreen tracker
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ priority_calculator.py # LightGBM priority scorer
â”‚   â”‚   â””â”€â”€ pattern_analyzer.py    # Audience heatmap builder
â”‚   â”œâ”€â”€ integrations/          # Platform publisher adapters
â”‚   â”œâ”€â”€ tasks/                 # Celery tasks (scheduler, publisher, monitor)
â”‚   â””â”€â”€ api/                   # REST route handlers
â”œâ”€â”€ alembic/                   # DB migrations
â”œâ”€â”€ tests/                     # pytest test suite
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

---

## Quick Start

### 1. Copy and fill `.env`

```bash
cp .env.example .env
# Then edit .env with your platform API keys
```

### 2. Start infrastructure

```bash
docker-compose up -d postgres redis
```

### 3. Set up Python environment

```bash
python -m venv venv
venv\Scripts\activate        # Windows
pip install -r requirements.txt
```

### 4. Run DB migrations

```bash
alembic upgrade head
```

### 5. Start the API server

```bash
uvicorn app.main:app --reload --port 8000
```

### 6. Start Celery workers (separate terminal)

```bash
celery -A app.tasks worker --loglevel=info -Q scheduler,publisher,monitor
celery -A app.tasks beat   --loglevel=info   # periodic tasks
```

### 7. Open the docs

- Swagger UI â†’ http://localhost:8000/docs  
- ReDoc      â†’ http://localhost:8000/redoc

---

## API Reference (key endpoints)

| Method | Path | Description |
|--------|------|-------------|
| `POST` | `/api/v1/queue/` | Add content to distribution queue |
| `GET`  | `/api/v1/queue/{id}` | Get queue entry status |
| `GET`  | `/api/v1/queue/user/{user_id}` | List all entries for a user |
| `DELETE` | `/api/v1/queue/{id}` | Cancel a pending entry |
| `POST` | `/api/v1/queue/{id}/approve` | Approve content for publishing |
| `GET`  | `/api/v1/schedule/optimal-time` | Get ML-predicted best posting time |
| `GET`  | `/api/v1/schedule/top-slots` | Top-5 ranked posting slots |
| `POST` | `/api/v1/schedule/publish-now/{id}` | Immediately publish |
| `POST` | `/api/v1/schedule/publish-async/{id}` | Enqueue via Celery |
| `GET`  | `/api/v1/analytics/performance/{user_id}` | Performance summary |
| `GET`  | `/api/v1/analytics/heatmap/{user_id}` | Audience engagement heatmap |
| `GET`  | `/api/v1/analytics/algorithm-changes/{platform}` | Algorithm change log |

---

## Running Tests

```bash
pytest -v
```

---

## Data ORBIT needs from other services

| Data | Source service | How it arrives |
|------|---------------|----------------|
| `content_id` + content payload | **FORGE** | REST call / shared Content DNA |
| `user_id` | Auth / SYNAPSE gateway | JWT token |
| Platform OAuth tokens | User onboarding flow | Stored in `platform_configs` table |
| Historical engagement data | **PULSE** | Populated into `audience_patterns` + `platform_performance` tables |
| Algorithm recommendations | **GENESIS** | Informs `PLATFORM_DEFAULTS` override |

---

## Integration with SYNAPSE Pillars

```
FORGE  â”€â”€â–º ORBIT â”€â”€â–º publish â”€â”€â–º PULSE (analytics feedback)
                  â”‚
                  â””â”€â”€â–º GENESIS (strategy feedback)
```

ORBIT sits between FORGE (content creation) and PULSE (analytics).  
Feed it `content_id` + `user_id` + `platforms[]` and it handles everything else.
